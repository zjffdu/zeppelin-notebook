{
  "paragraphs": [
    {
      "text": "%md\n\nThis recipe shows how to insert rows into a table so that downstream applications can read them.\n\nAs outlined in the first recipe Flink SQL operates on tables, that are stored in external systems. To publish results of a query for consumption by downstream applications, you write the results of a query into a table. This table can be read by Flink SQL, or directly by connecting to the external system that is storing the data (e.g. an ElasticSearch index.)\n\nThis example takes the `server_logs` tables, filters for client errors, and writes these logs into another table called `client_errors`. Any number of external systems could back the result table, including Apache Kafka, Apache Hive, ElasticSearch, JDBC, among many others. To keep this example self-contained, `client_errors` is of type `blackhole`: instead of actually writing the data to an external system, the table discards any rows written to it.\n\n## Screenshot of this tutorial note\n![FlinkSqlCook-Foudation-02InsertingIntoTables](https://user-images.githubusercontent.com/164491/122319203-e0c1d780-cf52-11eb-8ffb-eb61fd81541f.gif)",
      "user": "anonymous",
      "dateUpdated": "2021-06-17 10:01:10.940",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eThis recipe shows how to insert rows into a table so that downstream applications can read them.\u003c/p\u003e\n\u003cp\u003eAs outlined in the first recipe Flink SQL operates on tables, that are stored in external systems. To publish results of a query for consumption by downstream applications, you write the results of a query into a table. This table can be read by Flink SQL, or directly by connecting to the external system that is storing the data (e.g. an ElasticSearch index.)\u003c/p\u003e\n\u003cp\u003eThis example takes the \u003ccode\u003eserver_logs\u003c/code\u003e tables, filters for client errors, and writes these logs into another table called \u003ccode\u003eclient_errors\u003c/code\u003e. Any number of external systems could back the result table, including Apache Kafka, Apache Hive, ElasticSearch, JDBC, among many others. To keep this example self-contained, \u003ccode\u003eclient_errors\u003c/code\u003e is of type \u003ccode\u003eblackhole\u003c/code\u003e: instead of actually writing the data to an external system, the table discards any rows written to it.\u003c/p\u003e\n\u003ch2\u003eScreenshot of this tutorial note\u003c/h2\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://user-images.githubusercontent.com/164491/122319203-e0c1d780-cf52-11eb-8ffb-eb61fd81541f.gif\" alt\u003d\"FlinkSqlCook-Foudation-02InsertingIntoTables\" /\u003e\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614267802169_591840692",
      "id": "paragraph_1614267802169_591840692",
      "dateCreated": "2021-02-25 23:43:22.169",
      "dateStarted": "2021-06-17 10:01:10.941",
      "dateFinished": "2021-06-17 10:01:10.976",
      "status": "FINISHED"
    },
    {
      "text": "%md\n这个事例展示了怎么向表中插入数据以便下游应用使用这些数据。\n\n如第一个例子所示，Flink SQL 操作的表数据都是存储在外部系统上。通过将查询的结果写入表中来将结果提供给后续应用消费。这个表可以被 Flink SQL 读取，也可以通过直接与外部系统来保存数据（例如 ElasticSearch）。\n\n本例从 `server_logs` 表中过滤出客户端错误，然后将过滤出来的日志写入另一个叫 `client_errors` 的表，结果表可以持久化到任意外部系统中，包括 Apache Kafka， Apache Hive， ElasticSearch，JDBC， \n为了让这个例子能够在没有任何依赖的情况下跑起来，`client_errors` 表是 `blackhole` 类型，这个表将任何写入的数据丢弃掉而不是写入外部系统中。\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-18 15:07:40.048",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e这个事例展示了怎么向表中插入数据以便下游应用使用这些数据。\u003c/p\u003e\n\u003cp\u003e如第一个例子所示，Flink SQL 操作的表数据都是存储在外部系统上。通过将查询的结果写入表中来将结果提供给后续应用消费。这个表可以被 Flink SQL 读取，也可以通过直接与外部系统来保存数据（例如 ElasticSearch）。\u003c/p\u003e\n\u003cp\u003e本例从 \u003ccode\u003eserver_logs\u003c/code\u003e 表中过滤出客户端错误，然后将过滤出来的日志写入另一个叫 \u003ccode\u003eclient_errors\u003c/code\u003e 的表，结果表可以持久化到任意外部系统中，包括 Apache Kafka， Apache Hive， ElasticSearch，JDBC，\u003cbr /\u003e\n为了让这个例子能够在没有任何依赖的情况下跑起来，\u003ccode\u003eclient_errors\u003c/code\u003e 表是 \u003ccode\u003eblackhole\u003c/code\u003e 类型，这个表将任何写入的数据丢弃掉而不是写入外部系统中。\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615106952557_787103027",
      "id": "paragraph_1615106952557_787103027",
      "dateCreated": "2021-03-07 08:49:12.557",
      "dateStarted": "2021-03-18 15:07:40.048",
      "dateFinished": "2021-03-18 15:07:40.072",
      "status": "FINISHED"
    },
    {
      "text": "%flink.ssql\n\nDROP TABLE IF EXISTS server_logs;\n\nCREATE TABLE server_logs ( \n    client_ip STRING,\n    client_identity STRING, \n    userid STRING, \n    user_agent STRING,\n    log_time TIMESTAMP(3),\n    request_line STRING, \n    status_code STRING, \n    size INT\n) WITH (\n  \u0027connector\u0027 \u003d \u0027faker\u0027, \n  \u0027fields.client_ip.expression\u0027 \u003d \u0027#{Internet.publicIpV4Address}\u0027,\n  \u0027fields.client_identity.expression\u0027 \u003d  \u0027-\u0027,\n  \u0027fields.userid.expression\u0027 \u003d  \u0027-\u0027,\n  \u0027fields.user_agent.expression\u0027 \u003d \u0027#{Internet.userAgentAny}\u0027,\n  \u0027fields.log_time.expression\u0027 \u003d  \u0027#{date.past \u0027\u002715\u0027\u0027,\u0027\u00275\u0027\u0027,\u0027\u0027SECONDS\u0027\u0027}\u0027,\n  \u0027fields.request_line.expression\u0027 \u003d \u0027#{regexify \u0027\u0027(GET|POST|PUT|PATCH){1}\u0027\u0027} #{regexify \u0027\u0027(/search\\.html|/login\\.html|/prod\\.html|cart\\.html|/order\\.html){1}\u0027\u0027} #{regexify \u0027\u0027(HTTP/1\\.1|HTTP/2|/HTTP/1\\.0){1}\u0027\u0027}\u0027,\n  \u0027fields.status_code.expression\u0027 \u003d \u0027#{regexify \u0027\u0027(200|201|204|400|401|403|301){1}\u0027\u0027}\u0027,\n  \u0027fields.size.expression\u0027 \u003d \u0027#{number.numberBetween \u0027\u0027100\u0027\u0027,\u0027\u002710000000\u0027\u0027}\u0027\n);\n\nDROP TABLE IF EXISTS client_errors;\n\nCREATE TABLE client_errors (\n  log_time TIMESTAMP(3),\n  request_line STRING,\n  status_code STRING,\n  size INT\n)\nWITH (\n  \u0027connector\u0027 \u003d \u0027blackhole\u0027\n);\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-06-17 09:59:53.011",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Table has been dropped.\nTable has been created.\nTable has been dropped.\nTable has been created.\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614264261687_1276008773",
      "id": "paragraph_1614264261687_1276008773",
      "dateCreated": "2021-02-25 22:44:21.687",
      "dateStarted": "2021-06-17 09:59:53.013",
      "dateFinished": "2021-06-17 09:59:53.168",
      "status": "FINISHED"
    },
    {
      "text": "%flink.ssql\n\nINSERT INTO client_errors\nSELECT \n  log_time,\n  request_line,\n  status_code,\n  size\nFROM server_logs\nWHERE \n  status_code SIMILAR TO \u00274[0-9][0-9]\u0027\n  ",
      "user": "anonymous",
      "dateUpdated": "2021-06-17 10:00:04.186",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "ANGULAR",
            "data": "\u003ch1\u003eDuration: {{duration}} \u003c/h1\u003e\n"
          },
          {
            "type": "TEXT",
            "data": "Fail to run sql command: INSERT INTO client_errors\nSELECT \n  log_time,\n  request_line,\n  status_code,\n  size\nFROM server_logs\nWHERE \n  status_code SIMILAR TO \u00274[0-9][0-9]\u0027\njava.io.IOException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: f2e75e6b7f791f06da05be20005429ac)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.callInsertInto(FlinkSqlInterrpeter.java:531)\n\tat org.apache.zeppelin.flink.FlinkStreamSqlInterpreter.callInsertInto(FlinkStreamSqlInterpreter.java:97)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.callCommand(FlinkSqlInterrpeter.java:265)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.runSqlList(FlinkSqlInterrpeter.java:152)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.internalInterpret(FlinkSqlInterrpeter.java:110)\n\tat org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:47)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:849)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:741)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\n\tat org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:46)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: f2e75e6b7f791f06da05be20005429ac)\n\tat org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:125)\n\tat java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)\n\tat java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\n\tat java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)\n\tat org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:394)\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\n\tat java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)\n\tat org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$24(RestClusterClient.java:670)\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\n\tat java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)\n\tat org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:394)\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\n\tat java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)\n\tat java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)\n\tat java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)\n\t... 3 more\nCaused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled.\n\tat org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)\n\tat org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:123)\n\t... 24 more\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "FLINK JOB",
          "tooltip": "View in Flink web UI",
          "group": "flink",
          "values": [
            {
              "jobUrl": "http://localhost:8085#/job/f2e75e6b7f791f06da05be20005429ac"
            }
          ],
          "interpreterSettingId": "flink"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614264274230_70413416",
      "id": "paragraph_1614264274230_70413416",
      "dateCreated": "2021-02-25 22:44:34.231",
      "dateStarted": "2021-06-17 10:00:04.188",
      "dateFinished": "2021-06-17 10:02:27.685",
      "status": "ABORT"
    },
    {
      "text": "%flink.ssql(type\u003dupdate)\n\nSELECT \n  log_time,\n  request_line,\n  status_code,\n  size\nFROM server_logs\nWHERE \n  status_code SIMILAR TO \u00274[0-9][0-9]\u0027 order by log_time desc limit 10",
      "user": "anonymous",
      "dateUpdated": "2021-06-17 10:00:21.148",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "log_time": "string",
                      "request_line": "string",
                      "status_code": "string",
                      "size": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TABLE",
            "data": "log_time\trequest_line\tstatus_code\tsize\n2021-06-17T15:02:16\tPATCH /login.html /HTTP/1.0\t403\t3499143\n2021-06-17T15:02:16\tPOST cart.html HTTP/2\t401\t4045316\n2021-06-17T15:02:16\tPUT /login.html /HTTP/1.0\t400\t2047439\n2021-06-17T15:02:16\tGET cart.html /HTTP/1.0\t403\t2974365\n2021-06-17T15:02:16\tPATCH /login.html HTTP/2\t400\t82105\n2021-06-17T15:02:16\tPUT /prod.html /HTTP/1.0\t403\t8303026\n2021-06-17T15:02:16\tPATCH cart.html HTTP/2\t401\t5092700\n2021-06-17T15:02:16\tPUT cart.html HTTP/2\t403\t7528902\n2021-06-17T15:02:16\tGET /order.html HTTP/2\t400\t7079443\n2021-06-17T15:02:16\tPOST /search.html /HTTP/1.0\t401\t6985951\n"
          },
          {
            "type": "TEXT",
            "data": "Fail to run sql command: SELECT \n  log_time,\n  request_line,\n  status_code,\n  size\nFROM server_logs\nWHERE \n  status_code SIMILAR TO \u00274[0-9][0-9]\u0027 order by log_time desc limit 10\njava.io.IOException: Fail to run stream sql job\n\tat org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:172)\n\tat org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:105)\n\tat org.apache.zeppelin.flink.FlinkStreamSqlInterpreter.callInnerSelect(FlinkStreamSqlInterpreter.java:89)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.callSelect(FlinkSqlInterrpeter.java:495)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.callCommand(FlinkSqlInterrpeter.java:258)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.runSqlList(FlinkSqlInterrpeter.java:152)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.internalInterpret(FlinkSqlInterrpeter.java:110)\n\tat org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:47)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:849)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:741)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\n\tat org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:46)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: b341a45a0af9d9a915ce4422f595e020)\n\tat org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:125)\n\tat java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)\n\tat java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\n\tat java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)\n\tat org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:394)\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\n\tat java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)\n\tat org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$24(RestClusterClient.java:670)\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\n\tat java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)\n\tat org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:394)\n\tat java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\n\tat java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\n\tat java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\n\tat java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)\n\tat java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)\n\tat java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)\n\t... 3 more\nCaused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled.\n\tat org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)\n\tat org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:123)\n\t... 24 more\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "FLINK JOB",
          "tooltip": "View in Flink web UI",
          "group": "flink",
          "values": [
            {
              "jobUrl": "http://localhost:8085#/job/b341a45a0af9d9a915ce4422f595e020"
            }
          ],
          "interpreterSettingId": "flink"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614264279156_1925150514",
      "id": "paragraph_1614264279156_1925150514",
      "dateCreated": "2021-02-25 22:44:39.156",
      "dateStarted": "2021-06-17 10:00:21.150",
      "dateFinished": "2021-06-17 10:02:24.937",
      "status": "ABORT"
    },
    {
      "text": "%flink.ssql\n",
      "user": "anonymous",
      "dateUpdated": "2021-02-25 22:52:46.246",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614264766246_456910806",
      "id": "paragraph_1614264766246_456910806",
      "dateCreated": "2021-02-25 22:52:46.246",
      "status": "READY"
    }
  ],
  "name": "02 Inserting Into Tables",
  "id": "2G1GAHZY7",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0-SNAPSHOT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "flink-shared_process": [
      {
        "name": "duration",
        "object": "2 minutes 21 seconds",
        "noteId": "2G1GAHZY7",
        "paragraphId": "paragraph_1614264274230_70413416"
      }
    ]
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}